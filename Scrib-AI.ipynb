{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103416,
     "status": "ok",
     "timestamp": 1522492036928,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "hgrMhs_W91_S",
    "outputId": "e8b6ac55-5f29-4614-98b2-6c5814b4eaea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      ".vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On utilise le GPU, False story\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [00:29, 29.4MB/s]                           \n",
      "100%|██████████| 400000/400000 [00:17<00:00, 22806.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from io import open\n",
    "import torchtext.vocab as vocab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from math import floor\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA =torch.cuda.is_available()\n",
    "print('On utilise le GPU, '+str(USE_CUDA)+' story')\n",
    "\n",
    "#HYPERPARAMETRES TEST\n",
    "MIN_LENGTH_ARTICLE=10\n",
    "MIN_LENGTH_SUMMARY=5\n",
    "MAX_LENGTH_OUTPUT_GENERATE=50\n",
    "dim=100\n",
    "glove = vocab.GloVe(name='6B', dim=dim)\n",
    "\n",
    "#TOKENS\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pdMBRk1SAvOO"
   },
   "outputs": [],
   "source": [
    "def get_vocab(scribAPI, vocab_size, pad_token=0, unk_token=3, sos_token=1, eos_token=2):\n",
    "    \"\"\"\n",
    "    get word to index and index to word\n",
    "    :param scribAPI: ScribAI API object\n",
    "    :param vocab_size: size of the wanted vocab\n",
    "    \"\"\"\n",
    "    vocab = scribAPI.get_vocab(limit=vocab_size)\n",
    "    vocab_words = list(map(lambda x: x['word'], vocab))\n",
    "    vocab_counts = list(map(lambda x: x['count'], vocab))\n",
    "    word2index = {\"PAD\":pad_token, \"SOS\":sos_token,\"EOS\":eos_token,\"UNK\": unk_token}\n",
    "    word2count = {\"UNK\": 1}\n",
    "    index2word = {pad_token: \"PAD\", sos_token: \"SOS\", eos_token: \"EOS\", unk_token: \"UNK\"}\n",
    "    next_token = max([pad_token, unk_token, sos_token, eos_token]) + 1\n",
    "    for i, word in enumerate(vocab_words):\n",
    "        try:\n",
    "            g = glove.vectors[glove.stoi[word]]\n",
    "            word2index[word] = next_token\n",
    "            index2word[next_token] = word\n",
    "            word2count[word] = vocab_counts[i]\n",
    "            next_token += 1\n",
    "        except:\n",
    "            word2count[\"UNK\"] += 1\n",
    "    return word2index, word2count, index2word    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_7hBnb7j_gco"
   },
   "outputs": [],
   "source": [
    "def get_vect_from_word(word):\n",
    "    return glove.vectors[glove.stoi[word]]\n",
    "\n",
    "def tokenize_article_in_words(text_article):\n",
    "    sentences = [word_tokenize(t) for t in sent_tokenize(text_article)]\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(sentence)\n",
    "    return words\n",
    "\n",
    "def words_into_vect(words):\n",
    "    vector = None\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "          try:\n",
    "            vector = get_vect_from_word(word.lower())\n",
    "          except Exception:\n",
    "            vector = get_vect_from_word('unk')\n",
    "        elif i == 1:\n",
    "          try:\n",
    "            vector = torch.stack((vector, get_vect_from_word(word.lower())), 1)\n",
    "          except Exception:\n",
    "            vector = torch.stack((vector, get_vect_from_word('unk')), 1)\n",
    "        else:\n",
    "          try:\n",
    "            vector = torch.cat((vector, get_vect_from_word(word.lower())), 1)\n",
    "          except Exception:\n",
    "            vector = torch.cat((vector, get_vect_from_word('unk')), 1)\n",
    "    return vector\n",
    "\n",
    "def create_vocab_from_articles(A):\n",
    "    word2index = {\"PAD\":0, \"SOS\":1,\"EOS\":2,\"UNK\": 3}\n",
    "    word2count = {\"UNK\": 1}\n",
    "    index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "    n_words = 4  # Count default tokens\n",
    "    compteur_general=0\n",
    "    for i, article in enumerate(A):\n",
    "        words=tokenize_article_in_words(article)\n",
    "        for word in words:\n",
    "            compteur_general+=len(words)\n",
    "            if word not in word2index:\n",
    "                try:\n",
    "                    get_vect_from_word(word.lower())\n",
    "                    word2index[word] = n_words\n",
    "                    word2count[word] = 1\n",
    "                    index2word[n_words] = word\n",
    "                    n_words += 1\n",
    "                except Exception:\n",
    "                    word2count[\"UNK\"] += 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "    return word2index, word2count, index2word, compteur_general\n",
    "\n",
    "def create_ini_embedding(wordtoindex):\n",
    "    sample=wordtoindex.keys()\n",
    "    return words_into_vect(sample)\n",
    "\n",
    "  \n",
    "def pairs_and_filterpairs(articles,titles,word2index,m,n, seuil=3):\n",
    "    pairs=[]\n",
    "    compteur_train=0\n",
    "    for k, article in enumerate(articles):\n",
    "        compteur=0\n",
    "        words=tokenize_article_in_words(article)\n",
    "        words_target=tokenize_article_in_words(titles[k])\n",
    "        if len(words) >= m and len(words_target) >= n:\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word2index[word]\n",
    "                except Exception:\n",
    "                    compteur=compteur+1\n",
    "                if word=='UNK':\n",
    "                    compteur=compteur+1\n",
    "            for word in words_target:\n",
    "                try:\n",
    "                    word2index[word]\n",
    "                except Exception:\n",
    "                    compteur=compteur+1\n",
    "                if word=='UNK':\n",
    "                    compteur=compteur+1\n",
    "            if (100*float(compteur)/float(len(words)+len(words_target)))<seuil:\n",
    "              pairs.append([article,titles[k]])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def indexes_from_sentence(word2index, sentence):\n",
    "    ind=[]\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            ind.append(word2index[word])\n",
    "        except Exception:\n",
    "            ind.append(3)\n",
    "    return ind + [EOS_token]\n",
    "  \n",
    "def indexes_from_sentence_target(word2index, sentence,input_seq):\n",
    "    target_normal=[]\n",
    "    target_context=[]\n",
    "    target_pointer=[]\n",
    "    words_input=input_seq.split(' ')\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            target_normal.append(word2index[word])\n",
    "            target_pointer.append(0)\n",
    "            target_context.append(0)\n",
    "        except Exception:\n",
    "            target_normal.append(UNK_token)\n",
    "            try:\n",
    "              index_element = words_input.index(word)\n",
    "              target_context.append(index_element)\n",
    "              p=1\n",
    "            except Exception:\n",
    "              target_context.append(0)\n",
    "              p=0\n",
    "            target_pointer.append(p)\n",
    "    return target_normal + [EOS_token], target_context +[EOS_token], target_pointer\n",
    "  \n",
    "# Pad a with the PAD symbol\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "def pad_seq_l(seq, max_length):\n",
    "    seq += ['PAD' for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(batch_size, pairs,word2index):\n",
    "    input_seqs = []\n",
    "    input_seqs_letters=[]\n",
    "    target_seqs_voc = []\n",
    "    target_seqs_context = []\n",
    "    target_seqs_pointer = []\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs_letters.append(pair[0].split(' '))\n",
    "        input_seqs.append(indexes_from_sentence(word2index, pair[0]))\n",
    "        tn,tc,tp=indexes_from_sentence_target(word2index, pair[1],pair[0])\n",
    "        target_seqs_voc.append(tn)\n",
    "        target_seqs_context.append(tc)\n",
    "        target_seqs_pointer.append(tp)\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    seq_pairs = sorted(zip(input_seqs_letters, input_seqs, target_seqs_voc,target_seqs_context, target_seqs_pointer), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs_letters, input_seqs, target_seqs_voc,target_seqs_context, target_seqs_pointer = zip(*seq_pairs)\n",
    "    # For input and target sequences, get array of lengths and pad with 0s to max length\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs_voc]\n",
    "    target_padded_voc = [pad_seq(s, max(target_lengths)) for s in target_seqs_voc]\n",
    "    target_padded_context = [pad_seq(s, max(input_lengths)) for s in target_seqs_context]\n",
    "    target_padded_pointer = [pad_seq(s, max(target_lengths)) for s in target_seqs_pointer]\n",
    "    # Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var_voc = Variable(torch.LongTensor(target_padded_voc)).transpose(0, 1)\n",
    "    target_var_context= Variable(torch.LongTensor(target_padded_context)).transpose(0, 1)\n",
    "    target_var_pointer = Variable(torch.LongTensor(target_padded_pointer)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var_voc = target_var_voc.cuda()\n",
    "        target_var_context=target_var_context.cuda()\n",
    "        target_var_pointer=target_var_pointer.cuda()\n",
    "    \n",
    "    return input_var, input_lengths, target_var_voc, target_lengths,target_var_context, target_var_pointer,input_seqs_letters\n",
    "\n",
    "def random_batch_three(batch_size, articles, article_refs, articles2, article_refs2, scores, word2index):\n",
    "    input_seqs = []\n",
    "    input_seqs_2 = []\n",
    "    input_seqs_refs = []\n",
    "    input_seqs_refs_2 = []\n",
    "    target_seqs = []\n",
    "    # Choose random pairs\n",
    "    for i in range(batch_size):\n",
    "        k = random.randint(0, len(articles)-1)\n",
    "        article, article_ref = indexes_from_sentence(word2index, articles[k]), indexes_from_sentence(word2index, article_refs[k])\n",
    "        article2, article_ref2 = indexes_from_sentence(word2index, articles2[k]), indexes_from_sentence(word2index, article_refs2[k])\n",
    "        input_seqs.append(article)\n",
    "        input_seqs_2.append(article2)\n",
    "        input_seqs_refs.append(article_ref)\n",
    "        input_seqs_refs_2.append(article_ref2)\n",
    "        target_seqs.append(scores[k])\n",
    "    # Zip into pairs, sort by length (descending), unzip\n",
    "    order_batches1 = [k for k in range(len(input_seqs))]\n",
    "    order_batches2 = [k for k in range(len(input_seqs_2))]\n",
    "    seqs1 = sorted(zip(input_seqs, input_seqs_refs, order_batches1), key=lambda p: len(p[0]) + len(p[1]), reverse=True)\n",
    "    seqs2 = sorted(zip(input_seqs_2, input_seqs_refs_2, order_batches2), key=lambda p: len(p[0]) + len(p[1]), reverse=True)\n",
    "    input_seqs, input_seqs_refs, order_batches1 = zip(*seqs1)\n",
    "    input_seqs_2, input_seqs_refs_2, order_batches2 = zip(*seqs2)\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_lengths_2 = [len(s) for s in input_seqs_2]\n",
    "    input_seqs = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    input_seqs_2 = [pad_seq(s, max(input_lengths_2)) for s in input_seqs_2]\n",
    "    input_seqs_refs_length = [len(s) for s in input_seqs_refs]\n",
    "    input_seqs_refs_length_2 = [len(s) for s in input_seqs_refs_2]\n",
    "    input_seqs_refs = [pad_seq(s, max(input_seqs_refs_length)) for s in input_seqs_refs]\n",
    "    input_seqs_refs_2 = [pad_seq(s, max(input_seqs_refs_length_2)) for s in input_seqs_refs_2]\n",
    "    target_seqs = list(target_seqs)\n",
    "\n",
    "    return input_seqs, input_lengths, input_seqs_refs, input_seqs_refs_length, order_batches1, input_seqs_2, input_lengths_2, input_seqs_refs_2, input_seqs_refs_length_2, order_batches2, target_seqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "91kD-PZHQz04"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6888,
     "status": "ok",
     "timestamp": 1522492052538,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "pWdW1GycRtnC",
    "outputId": "95ab3595-36f4-4578-f779-bed377e3eec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charge Vocab & pretrain embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Charge Vocab & pretrain embedding\")\n",
    "wordtoindex, wordtocount, indextoword = get_vocab(scribAPI, 10000)\n",
    "pretrained_weight=create_ini_embedding(wordtoindex).transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9W6CVlfZQ0iU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3882,
     "status": "ok",
     "timestamp": 1522492057136,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "2tzGwOdqLUOw",
    "outputId": "ce4586de-908b-4e83-eb24-71bae8084c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charge Batch\n"
     ]
    }
   ],
   "source": [
    "print(\"Charge Batch\")\n",
    "articles, titles = cnn_dataset.get_batch(batch_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwnYBH2EAnus"
   },
   "source": [
    "Exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1522492058152,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "W9sNHjLH_rBo",
    "outputId": "665fb9bd-ba9b-43a5-eaff-60aa979374e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexique of 9976 words\n",
      "Create pairs\n",
      "27 pairs\n",
      "27346719\n",
      "121 tokens \"UNK\", represent 0.0000 \n"
     ]
    }
   ],
   "source": [
    "print('Lexique of %d words' % len(indextoword))\n",
    "print(\"Create pairs\")\n",
    "pairs_train=pairs_and_filterpairs(articles,titles,wordtoindex, MIN_LENGTH_ARTICLE,MIN_LENGTH_SUMMARY, seuil=10)\n",
    "print('%d pairs' % len(pairs_train))\n",
    "articles_finals=[]\n",
    "for i in pairs_train:\n",
    "  articles_finals.append(i[0])\n",
    "wti, wtc, itw, c=create_vocab_from_articles(articles_finals)\n",
    "print(c)\n",
    "print('%d tokens \"UNK\", represent %.4f ' % (wtc[\"UNK\"], float(wtc[\"UNK\"])/float(c)))\n",
    "#random_batch(50, pairs_train, wordtoindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQ5wrd6d5JuA"
   },
   "source": [
    "# Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cMlkxrWQBsKM"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_size, embed_size, hidden_size,pretrained_weight, n_layers=1, dropout=0.5):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \"\"\"\n",
    "        :param input_size\n",
    "        :param embed_size\n",
    "        :param hidden_size\n",
    "        :param pretrained_weight\n",
    "        :param n_layers\n",
    "        :param dropout\n",
    "        \"\"\"\n",
    "        # Define parameters\n",
    "        self.input_size = input_size  # V Taille Vocabulary (can be different, here not)\n",
    "        self.hidden_size = hidden_size  # H\n",
    "        self.embed_size = embed_size  # E\n",
    "        self.n_layers = n_layers  # L (1 per default)\n",
    "        self.dropout = dropout  # 0.5 per default\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)  # Init (V,E)\n",
    "        self.embedding.weight = nn.Parameter(pretrained_weight) #Init with glove\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout,\n",
    "                          bidirectional=True)  # Init (E,H,L, Bidirectionnel!)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        \"\"\"\n",
    "        :param input_seqs:\n",
    "            Variable of shape (T,B), T is the number of words in the longuest sentence, B is Batchsize. Contening the indexing of the words reference to the voc\n",
    "        :param input_lengths:\n",
    "            list of integers (len=B) which reprensents the number of words in sequence for each batch. Normally Max(input_lengths)=T\n",
    "        :param hidden:\n",
    "            initial state of GRU\n",
    "        :returns:\n",
    "            GRU outputs in shape (T,B,H)\n",
    "            last hidden stat of RNN(L*bidirectionnal,B,H)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seqs)  # (T,B,E)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)  # cf doc pytorch : take embedding and input_length. Ready to go\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n",
    "        \n",
    "        return outputs, hidden  # (T,B,H),(L*bidirectionnal,B,H) | bidirectionnal=2 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SWghUwbiB7zm"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    \n",
    "    def __init__(self, method, hidden_size, temporal=False):\n",
    "        super(Attn, self).__init__()\n",
    "        \"\"\"\n",
    "        :param method\n",
    "        :param hidden_size\n",
    "        :param temporal\n",
    "        \"\"\"\n",
    "        # Define parameters\n",
    "        self.method = method  # 2 methods cf publi\n",
    "        self.hidden_size = hidden_size  # H\n",
    "        self.temporal = temporal  # Temporal attention encoder\n",
    "        self.softmax = nn.Softmax()\n",
    "        # Define layers\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)  # Init(2*H,H)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.normal_(mean=0, std=stdv)\n",
    "\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, E_history=None):\n",
    "        \"\"\"\n",
    "        :param hidden:\n",
    "            (B,H)\n",
    "        :param encoder_outputs:\n",
    "            (T,B,H) can be also hidden decoder accumulation over time (t+1,B,H), depends on attention encoder or decoder\n",
    "        :param E_history:\n",
    "           Encoder history use only if intra temporal attention. Init with None, then (t,B,T)\n",
    "        :returns:\n",
    "            attn_energies which is alpha\n",
    "        \"\"\"\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(H, encoder_outputs)\n",
    "        if self.temporal:\n",
    "            if E_history is None:\n",
    "                E_history = attn_energies.unsqueeze(0)\n",
    "            else:\n",
    "                E_history = torch.cat([E_history, attn_energies.unsqueeze(0)], 0)\n",
    "                hist = E_history.view(-1, this_batch_size * max_len).t()\n",
    "                attn_energies = self.softmax(hist)[:, -1].contiguous().view(this_batch_size, max_len)\n",
    "            return F.softmax(attn_energies).unsqueeze(1), E_history\n",
    "        else:\n",
    "          # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "          return F.softmax(attn_energies).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        \"\"\"\n",
    "        :param hidden\n",
    "        :param encoder_output\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        elif self.method == 'concat':\n",
    "            input=torch.cat([hidden, encoder_output], 2)\n",
    "            energy = F.tanh(self.attn(input))  # [B*T*2H]->[B*T*H]\n",
    "            energy = energy.transpose(2, 1)  # [B*H*T]\n",
    "            v = self.v.repeat(encoder_output.data.shape[0], 1).unsqueeze(1)  # [B*1*H]\n",
    "            energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "            return energy.squeeze(1)  # [B*T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0NPia7FlCMZ8"
   },
   "outputs": [],
   "source": [
    "class DecoderStep(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, embed_size, output_size, n_layers, temporal=True, de_att_bol=True, point_bol=True, attention_bol=True, dropout_p=0.1):\n",
    "        super(DecoderStep, self).__init__()\n",
    "        \"\"\"\n",
    "        :param hidden_size\n",
    "        :param embed_size\n",
    "        :param output_size\n",
    "        :param n_layers\n",
    "        :param temporal\n",
    "        :param de_att_bol\n",
    "        :param point_bol\n",
    "        :param attention_bol\n",
    "        :param dropout_p   \n",
    "        \"\"\"\n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size  # H\n",
    "        self.output_size = output_size  # V\n",
    "        self.n_layers = n_layers  # L\n",
    "        self.dropout_p = dropout_p  # 0.1 per default\n",
    "        self.temporal = temporal  # bolean to use intra temporal attention on input sequence cf. Temporal attention model for neural machine translation. arXiv preprint arXiv:1608.02927, 2016\n",
    "        self.decoder_attention_bolean = de_att_bol  # bolean to use intra decoder attention\n",
    "        self.pointer_boloan = point_bol\n",
    "        self.attention_bolean=attention_bol\n",
    "        self.embed_size=embed_size\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, embed_size) # Init(V,E)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        if self.attention_bolean:\n",
    "          self.attn_encoder = Attn('concat', hidden_size, temporal) # Init(methode score, H, bolean temporal), cf class\n",
    "          if self.decoder_attention_bolean:\n",
    "            self.attn_decoder = Attn('concat', hidden_size, temporal=False)\n",
    "            self.gru = nn.GRU(2*hidden_size + embed_size, hidden_size, n_layers, dropout=dropout_p)# init(3*H+E,H,L) \n",
    "            self.out = nn.Linear(hidden_size * 3, output_size)  # Wout(3H,V) case [1] and [2]\n",
    "            self.out_proba = nn.Linear(hidden_size * 3, 1)\n",
    "          else:\n",
    "            self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout_p)# init(2*H+E,H,L) \n",
    "            self.out = nn.Linear(hidden_size * 2, output_size)  # Wout(2H,V) case [1] and [2]\n",
    "            self.out_proba = nn.Linear(hidden_size * 2, 1)\n",
    "        else:\n",
    "          self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout_p)# init(E,H,L) \n",
    "          self.out = nn.Linear(hidden_size, output_size)  # Wout(H,V) case [1] and [2]\n",
    "        \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs,E_hist,t, hd_history, input_batches):\n",
    "        \"\"\"\n",
    "        :param word_input:\n",
    "            tensor with SOS_Token length B\n",
    "        :param last_hidden:\n",
    "            Last hidden of the decoder, initialization with last hidden encoder (L,B,H)\n",
    "        :param encoder_outputs:\n",
    "            encoder output (T,B,H)\n",
    "        :param E_hist:\n",
    "            Encoder history use only if intra temporal attention. Init with None, then (1,B,T)\n",
    "        :param t:\n",
    "            number of time generate words [0 to max_length sequence]\n",
    "        :param hd_history:\n",
    "            hidden decoder accumulation over time (t+1,B,H)\n",
    "        :param input_batches:\n",
    "            input de l'encoder to be able to point to the entry (V,B)\n",
    "        :returns:\n",
    "            output decoder : max proba is the word to generate (B,V)\n",
    "            hidden : will be last hidden next time\n",
    "            alpha : to plot during the evaluation\n",
    "            E_history : will be E_hist next time\n",
    "            hd_history : will be hd_history next time\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, word_input.size(0), -1)  # (1,B,E)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        if self.attention_bolean:\n",
    "          # Calculate attention weights -temporal or not- of encoder (alpha) and apply to encoder outputs (context_encoder)\n",
    "          if self.temporal:\n",
    "              alpha, E_history = self.attn_encoder(last_hidden[-1], encoder_outputs, E_hist)  # (B,1,T) (1,B,T)\n",
    "          else:\n",
    "              E_history = None  # None\n",
    "              alpha = self.attn_encoder(last_hidden[-1], encoder_outputs)  # (B,1,T) alpha will be use later\n",
    "          context_encoder = alpha.bmm(encoder_outputs.transpose(0, 1))  # (B,1,H)\n",
    "          context_encoder = context_encoder.transpose(0, 1)  # (1,B,H) context with the encoder.\n",
    "          # attention on decoder and RNN input\n",
    "          if self.decoder_attention_bolean:\n",
    "              if t:  # Recurrence\n",
    "                  alpha_d = self.attn_decoder(last_hidden[-1], hd_history)  # (B,1,t)\n",
    "                  context_decoder = alpha_d.bmm(hd_history.transpose(0, 1))  # (1,B,H)\n",
    "                  context_decoder = context_decoder.transpose(0, 1)  # (1,H,B)\n",
    "                  hd_history = torch.cat([hd_history, last_hidden[-1].unsqueeze(0)], dim=0)  # (t+1,B,H)\n",
    "              else:  # Initialisation\n",
    "                  context_decoder = Variable(torch.zeros(1, word_embedded.size()[1], self.hidden_size))  # init to zero\n",
    "                  if USE_CUDA:\n",
    "                    context_decoder=context_decoder.cuda()\n",
    "              rnn_input_old = torch.cat((word_embedded, context_encoder), 2)\n",
    "              rnn_input = torch.cat((rnn_input_old, context_decoder), 2)  # (1,B,H+E+t) idem\n",
    "          else:\n",
    "            # Combine embedded input word and attended context, run through RNN\n",
    "            rnn_input = torch.cat((word_embedded, context_encoder), 2)\n",
    "        else:\n",
    "          rnn_input=word_embedded\n",
    "        #RNN\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,H)->(B,H)\n",
    "        if self.attention_bolean:\n",
    "          context_encoder = context_encoder.squeeze(0)\n",
    "          output_concat=torch.cat((output, context_encoder),1)\n",
    "          #pointer\n",
    "          if self.pointer_boloan:\n",
    "            if self.decoder_attention_bolean:\n",
    "                context_decoder = context_decoder.squeeze(0)\n",
    "                output_concat_v2=torch.cat((output_concat, context_decoder),1)\n",
    "                p_yt_sachant_paspointer = F.log_softmax(self.out(output_concat_v2))\n",
    "                p_pointer=F.sigmoid(self.out_proba(output_concat_v2))\n",
    "            else:\n",
    "                p_yt_sachant_paspointer = F.log_softmax(self.out(output_concat))\n",
    "                p_pointer=F.sigmoid(self.out_proba(output_concat))\n",
    "            #Calcul proba finale\n",
    "            alpha_interet = alpha.squeeze(1)  # (T,B)\n",
    "            output=[p_yt_sachant_paspointer,alpha_interet, p_pointer]   \n",
    "          else:\n",
    "            if self.decoder_attention_bolean:\n",
    "              context_decoder = context_decoder.squeeze(0)\n",
    "              output_concat_v2=torch.cat((output_concat, context_decoder),1)\n",
    "              output = F.log_softmax(self.out(output_concat_v2))\n",
    "            else:\n",
    "              output = F.log_softmax(self.out(output_concat))\n",
    "        else:\n",
    "            alpha=None\n",
    "            E_history=None\n",
    "            output = F.log_softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden, alpha, E_history, hd_history\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTYnXeiP7i3o"
   },
   "source": [
    "# Reward with human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Fvy73F6TCwwQ"
   },
   "outputs": [],
   "source": [
    "class RewardPredictor(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, embed_size, hidden_size, indextoword,\n",
    "                 wordtoindex, num_layer_encoder=1,\n",
    "                 dropout=0.5, SOS_token=1, EOS_token=2, PAD_token=0, batch_size=5, article_max_size=500):\n",
    "    super(RewardPredictor, self).__init__()\n",
    "    \"\"\"\n",
    "    :param input_size:\n",
    "    :param embed_size:\n",
    "    :param hidden_size:\n",
    "    :param indextoword:\n",
    "    :param wordtoindex\n",
    "    :param num_layer_encoder:\n",
    "    :param dropout:\n",
    "    :param SOS_token:\n",
    "    :param EOS_token:\n",
    "    :param PAD_token:\n",
    "    :param batch_size:\n",
    "    :param article_max_size:\n",
    "    \"\"\"\n",
    "    super(RewardPredictor, self).__init__()\n",
    "    self.encoder = EncoderRNN(input_size, embed_size, hidden_size,pretrained_weight, n_layers=num_layer_encoder, dropout=dropout)\n",
    "    \n",
    "    # conv2d: Hout = floor((Hin−kernel_size[0])/stride[0]+1)\n",
    "    # Maxpool: Hout = floor((Hin-kernel_size[0])/stride[0]+1)\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(1, 8, kernel_size=10, stride=4, padding=0, dilation=1)\n",
    "    # Max pooling of size 2 kernel_size=2, stride=2, padding=0, dilation=1\n",
    "    T, H = floor((article_max_size - 10)/4 + 1), floor((2*hidden_size - 10)/4 + 1)\n",
    "    T, H = floor((T - 2)/2 + 1), floor((H - 2)/2 + 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(8, 8, kernel_size=7, stride=3)\n",
    "    # Max pooling of size 2 kernel_size=2, stride=2, padding=0, dilation=1\n",
    "    T, H = floor((T-7)/3 + 1), floor((H-7)/3 + 1)\n",
    "    T, H = floor((T-2)/2 + 1), floor((T-2)/2 + 1)\n",
    "    \n",
    "    self.conv3 = nn.Conv2d(8, 8, kernel_size=5, stride=2)\n",
    "    # Max pooling of size 2 kernel_size=2, stride=2, padding=0, dilation=1\n",
    "    T, H = floor((T-5)/2 + 1), floor((H-5)/2 + 1)\n",
    "    # T, H = floor((T-2)/2 + 1), floor((T-2)/2 + 1)  # enlevé car pass assez de dim\n",
    "    print(2*hidden_size, article_max_size)\n",
    "    print(\"size reward\", 16*T*H, 50)\n",
    "    self.out_layer1 = nn.Linear(8 * T * H-8, 50)\n",
    "    self.out_layer2 = nn.Linear(50, 1)\n",
    "    \n",
    "    self.indextoword = indextoword\n",
    "    self.wordtoindex = wordtoindex\n",
    "    self.SOS_token = SOS_token\n",
    "    self.EOS_token = EOS_token\n",
    "    self.PAD_token = PAD_token\n",
    "    self.batch_size = batch_size\n",
    "    self.article_max_size = article_max_size\n",
    "\n",
    "    if USE_CUDA:\n",
    "        self.encoder.cuda()\n",
    "        self.conv1.cuda()\n",
    "        self.conv2.cuda()\n",
    "        self.out_layer1.cuda()\n",
    "        self.out_layer2.cuda()\n",
    "  \n",
    "  def forward(self, input_seq_ref, input_seq_ref_length, input_seq, input_seq_length):\n",
    "    \"\"\"\n",
    "    Evaluate the reward function\n",
    "    :param input_seq_ref: ref sequence (article)\n",
    "    :param input_seq: summary sequence\n",
    "    \"\"\"\n",
    "    # Run through encoder\n",
    "    # print(input_seq_ref, input_seq_ref_length)\n",
    "    input_seq = torch.squeeze(input_seq, 1)\n",
    "    input_seq_ref = torch.squeeze(input_seq_ref, 1)\n",
    "    #input_seq_ref = torch.transpose(input_seq_ref, 0, 1)\n",
    "    #input_seq = torch.transpose(input_seq, 0, 1)\n",
    "\n",
    "    # print(input_seq_ref)\n",
    "    encoder_outputs, encoder_hidden = self.encoder(input_seq, input_seq_length, None)\n",
    "    encoder_ref_outputs, encoder_ref_hidden = self.encoder(input_seq_ref, input_seq_ref_length, None)\n",
    "    input_metric = F.pad(encoder_outputs, \n",
    "                         (0, 0, 0, 0, 0, self.article_max_size - encoder_outputs.size(0)), \n",
    "                         \"constant\", self.PAD_token)\n",
    "    input_metric_ref = F.pad(encoder_ref_outputs, \n",
    "                         (0, 0, 0, 0, 0, self.article_max_size - encoder_ref_outputs.size(0)), \n",
    "                         \"constant\", self.PAD_token)\n",
    "    input_metric = torch.transpose(input_metric, 0, 1)\n",
    "    input_metric_ref = torch.transpose(input_metric_ref, 0, 1)\n",
    "    print(\"sortie\", input_metric)\n",
    "    compare_vector = torch.cat((input_metric, input_metric_ref), dim=1)\n",
    "    compare_vector = torch.unsqueeze(compare_vector, 1)  # Add channel dimension\n",
    "    print(\"avant\")\n",
    "    print(compare_vector.size())\n",
    "    compare_vector = F.max_pool2d(F.relu(self.conv1(compare_vector)), 2)\n",
    "    print(\"après C1\")\n",
    "    print(compare_vector.size())\n",
    "    compare_vector = F.max_pool2d(F.relu(self.conv2(compare_vector)), 2)\n",
    "    print(\"après C2\")\n",
    "    print(compare_vector.size())\n",
    "    # compare_vector = F.max_pool2d(F.relu(self.conv3(compare_vector)), 2)\n",
    "    compare_vector = F.relu(self.conv3(compare_vector))  # Pas de pooling ici, pas assez de dimension apparement\n",
    "    print(\"après C3\")\n",
    "    print(compare_vector.size())\n",
    "    compare_vector = compare_vector.view(self.batch_size, 1, -1)\n",
    "    print(\"maitenant\")\n",
    "    print(compare_vector.size())\n",
    "    compare_vector = self.out_layer1(compare_vector)\n",
    "    return torch.squeeze(self.out_layer2(compare_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RynS4So_8D84"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "khpfUtpsbtG2"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTrainer(nn.Module):\n",
    "  \"\"\"\n",
    "  Model used for the human preferences metric\n",
    "  \"\"\"\n",
    "  def __init__(self, attn_model, hidden_size, embed_size, n_layers, wordtoindex, indextoword,\n",
    "               n_epochs, encoder_temporal,decoder_attention_bol, pairs_train,\n",
    "                pointeur_bolean, attention_bolean, tie_weights_bolean, num_layer_encoder=1,\n",
    "                num_layer_decoder=1,dropout=0.5, SOS_token=1, EOS_token=2, PAD_token=0, batch_size=5,\n",
    "                clip=50,learning_rate = 0.0001, article_max_size=500, decoder_learning_ratio=5,\n",
    "                plot_every=20, print_every = 100, evaluate_every = 1000, max_length=100):\n",
    "    super(Seq2SeqTrainer, self).__init__()\n",
    "    \"\"\"\n",
    "    :param input_size:\n",
    "    :param embed_size:\n",
    "    :param hidden_size:\n",
    "    :param output_size:\n",
    "    :param indextoword:\n",
    "    :param wordtoindex\n",
    "    :param num_layer_encoder:\n",
    "    :param num_layer_decoder:\n",
    "    :param dropout:\n",
    "    :param SOS_token:\n",
    "    :param EOS_token:\n",
    "    :param PAD_token:\n",
    "    :param batch_size:\n",
    "    :param article_max_size:\n",
    "    \"\"\"\n",
    "    # Configure models\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout = dropout\n",
    "    self.batch_size = batch_size\n",
    "    self.embed_size=embed_size\n",
    "    # Configure training/optimization\n",
    "    self.clip = clip\n",
    "    self.learning_rate = learning_rate\n",
    "    self.decoder_learning_ratio = decoder_learning_ratio\n",
    "    self.n_epochs = n_epochs\n",
    "    self.plot_every = plot_every\n",
    "    self.print_every = print_every\n",
    "    self.evaluate_every = evaluate_every\n",
    "    #config type model\n",
    "    self.encoder_temporal=encoder_temporal\n",
    "    self.decoder_attention_bol=decoder_attention_bol\n",
    "    self.pointeur_bolean=pointeur_bolean\n",
    "    self.attention_bolean=attention_bolean\n",
    "    self.tie_weights_bolean=tie_weights_bolean\n",
    "    self.indextoword=indextoword\n",
    "    self.wordtoindex=wordtoindex\n",
    "    self.pairs_train=pairs_train\n",
    "    # Initialize models\n",
    "    self.encoder = EncoderRNN(len(indextoword), embed_size, hidden_size,pretrained_weight, n_layers=n_layers, dropout=dropout)\n",
    "    self.decoder = DecoderStep(hidden_size, embed_size,len(indextoword), n_layers, dropout_p=dropout, temporal=encoder_temporal,de_att_bol=decoder_attention_bol,point_bol=pointeur_bolean,attention_bol=attention_bolean )\n",
    "    self.max_length=max_length\n",
    "    # Move models to GPU\n",
    "    if USE_CUDA:\n",
    "        self.encoder.cuda()\n",
    "        self.decoder.cuda()\n",
    "\n",
    "\n",
    "  def train_step(self, input_batches, input_lengths, target_batches, target_lengths, encoder_optimizer, decoder_optimizer, criterion, target_context, target_pointer,E_hist=None):\n",
    "    \"\"\"\n",
    "      \n",
    "      \n",
    "      \n",
    "    \"\"\"\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    total_loss = 0 # Added onto for each word\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = self.encoder(input_batches, input_lengths, None)\n",
    "    # Share embedding\n",
    "    self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "    # Optionally tie weights as in:\n",
    "    # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "    if self.tie_weights_bolean:\n",
    "      # Depend of the configuration you choose, the dimmensions must be : E= H or 2*H ou 3*H\n",
    "      decoder.out.weight = encoder.embedding.weight\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:self.decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, self.batch_size, self.decoder.output_size))\n",
    "    all_decoder_outputs_context = Variable(torch.zeros(max_target_length, self.batch_size, input_lengths[0]))\n",
    "    all_decoder_outputs_pointer = Variable(torch.zeros(max_target_length, self.batch_size,1))\n",
    "    hidden_history_decoder = decoder_hidden[-1].unsqueeze(0)\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        all_decoder_outputs_context=all_decoder_outputs_context.cuda()\n",
    "        all_decoder_outputs_pointer=all_decoder_outputs_pointer.cuda()\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn, E_hist,hidden_history_decoder = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs,E_hist,t, hidden_history_decoder, input_batches)\n",
    "        if self.pointeur_bolean:\n",
    "          all_decoder_outputs[t] = decoder_output[0]\n",
    "          all_decoder_outputs_context[t] = decoder_output[1]\n",
    "          all_decoder_outputs_pointer[t] = decoder_output[2]\n",
    "        else:        \n",
    "          all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # Next input is current target\n",
    "    # Loss calculation and backpropagation\n",
    "    if self.pointeur_bolean:\n",
    "      #1er loss : le pointeur\n",
    "      all_decoder_outputs_pointer=all_decoder_outputs_pointer.squeeze(2)\n",
    "      loss1=criterion(all_decoder_outputs_pointer,torch.max(target_pointer, 1)[1])\n",
    "      if 1 in target_pointer.cpu().data.numpy():\n",
    "        #update tous les poids\n",
    "        loss2 = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths)\n",
    "        total_loss= sum([loss1,loss2])\n",
    "      else:\n",
    "        loss2 = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths) \n",
    "        #loss sur l'attention en pointer, ca ne va pas tuer l'attention ?\n",
    "        #loss3 = \n",
    "        total_loss= sum([loss1,loss2])\n",
    "    else:    \n",
    "      total_loss = masked_cross_entropy(\n",
    "          all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "          target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "          target_lengths)\n",
    "    total_loss.backward()\n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(self.encoder.parameters(), self.clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(self.decoder.parameters(), self.clip)\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return total_loss.data[0], ec, dc\n",
    "        \n",
    "      \n",
    "  def evaluate(self, input_seq):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    input_lengths = [len(input_seq.split())]\n",
    "    input_seqs = [indexes_from_sentence(wordtoindex, input_seq)]\n",
    "    input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    words_input=input_seq.split(' ')\n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "    # Set to not-training mode to disable dropout\n",
    "    self.encoder.train(False)\n",
    "    self.decoder.train(False)\n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = self.encoder(input_batches, input_lengths, None)\n",
    "    # Share embedding\n",
    "    self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "    # Optionally tie weights as in:\n",
    "    # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "    if self.tie_weights_bolean:\n",
    "      # Depend of the configuration you choose, the dimmensions must be : E= H or 2*H ou 3*H\n",
    "      self.decoder.out.weight = self.encoder.embedding.weight\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:self.decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    hidden_history_decoder = decoder_hidden[-1].unsqueeze(0)\n",
    "    E_hist = None\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    \n",
    "    decoder_attentions = torch.zeros(max(input_lengths) + 1, max(input_lengths) + 1)\n",
    "    # Run through decoder\n",
    "    for di in range(self.max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention, E_hist,hidden_history_decoder = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs,E_hist,di, hidden_history_decoder,input_batches\n",
    "        )\n",
    "        if self.attention_bolean:\n",
    "          if self.pointeur_bolean:\n",
    "            [decoder_output_voc, decoder_output_context, decoder_output_pointer]=decoder_output\n",
    "            decoder_attentions[di,:decoder_attention.size(2)] += decoder_output_context.squeeze(0).cpu().data\n",
    "        if self.pointeur_bolean:\n",
    "          [decoder_output_voc, decoder_output_context, decoder_output_pointer]=decoder_output\n",
    "          if decoder_output_pointer.cpu().data.numpy()[0][0]>0.5:\n",
    "            topv, topi = decoder_output_context.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            topv2, topi2 = decoder_output_context.data.topk(2)\n",
    "            ni2 = topi2[0][0]\n",
    "            if len(decoded_words)>=2:\n",
    "              if decoded_words[-1]==decoded_words[-2]:\n",
    "                if words_input[ni]==decoded_words[-1]:\n",
    "                  ni=ni2\n",
    "            decoded_words.append(words_input[ni])\n",
    "            try:\n",
    "              ni=wordtoindex[words_input[ni]]\n",
    "            except Exception:\n",
    "              ni=3\n",
    "            decoder_input = Variable(torch.LongTensor([ni]))\n",
    "          else:\n",
    "            topv, topi = decoder_output_voc.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            topv2, topi2 = decoder_output_context.data.topk(2)\n",
    "            ni2 = topi2[0][0]\n",
    "            if ni == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "              if len(decoded_words)>=2:\n",
    "                if decoded_words[-1]==decoded_words[-2]:\n",
    "                  if indextoword[ni]==decoded_words[-1]:\n",
    "                    ni=ni2              \n",
    "              decoded_words.append(self.indextoword[ni])\n",
    "              decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        else:        \n",
    "          # Choose top word from output\n",
    "          topv, topi = decoder_output.data.topk(1)\n",
    "          ni = topi[0][0]\n",
    "          topv2, topi2 = decoder_output.data.topk(2)\n",
    "          ni2 = topi2[0][0]\n",
    "          if ni == EOS_token:\n",
    "              decoded_words.append('<EOS>')\n",
    "              break\n",
    "          else:\n",
    "              if len(decoded_words)>=2:\n",
    "                if decoded_words[-1]==decoded_words[-2]:\n",
    "                  if indextoword[ni]==decoded_words[-1]:\n",
    "                    ni=ni2\n",
    "              decoded_words.append(self.indextoword[ni])\n",
    "              decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA:\n",
    "          decoder_input = decoder_input.cuda()\n",
    "    # Set back to training mode\n",
    "    self.encoder.train(True)\n",
    "    self.decoder.train(True)\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]\n",
    "  \n",
    "  def evaluate_and_show_attention(self, input_sentence, target_sentence=None):\n",
    "    output_words, attentions = self.evaluate(input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    \n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "  \n",
    "  \n",
    "  def evaluate_randomly(self):\n",
    "    [input_sentence, target_sentence] = random.choice(pairs_train)\n",
    "    self.evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "    \n",
    "  \n",
    "  \n",
    "  def train_all(self):\n",
    "    epoch=0\n",
    "    # Initialize optimizers and criterion\n",
    "    encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.learning_rate)\n",
    "    decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.learning_rate * self.decoder_learning_ratio)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Keep track of time elapsed and running averages\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # Reset every print_every\n",
    "    plot_loss_total = 0 # Reset every plot_every\n",
    "    # Begin!\n",
    "    ecs = []\n",
    "    dcs = []\n",
    "    eca = 0\n",
    "    dca = 0\n",
    "    while epoch < self.n_epochs:\n",
    "        print(\"Epoch:\", epoch)\n",
    "        epoch += 1\n",
    "        # Get training data for this cycle\n",
    "        input_batches, input_lengths, target_batches, target_lengths,target_context, target_pointer,_ = random_batch(self.batch_size,self.pairs_train, self.wordtoindex)\n",
    "        # Run the train function\n",
    "        loss, ec, dc = self.train_step(\n",
    "            input_batches, input_lengths, target_batches, target_lengths,\n",
    "            encoder_optimizer, decoder_optimizer, criterion,target_context, target_pointer\n",
    "        )\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        eca += ec\n",
    "        dca += dc\n",
    "        if epoch == 5:\n",
    "            self.evaluate_randomly()\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "            show_losses(ecs,dcs,plot_losses)\n",
    "        if epoch % evaluate_every == 0:\n",
    "            self.evaluate_randomly()\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            ecs.append(eca / plot_every)\n",
    "            dcs.append(dca / plot_every)\n",
    "            eca = 0\n",
    "            dca = 0\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "            \n",
    "  def train_master_piece(self, input_batches, input_lengths, encoder_optimizer, decoder_optimizer,input_letters,reward_predictor):\n",
    "    \n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    total_loss = 0 # Added onto for each word\n",
    "\n",
    "          \n",
    "    # Run words through encoder\n",
    "    if torch.cuda.is_available():\n",
    "        input_batches = input_batches.cuda()\n",
    "    encoder_outputs, encoder_hidden = self.encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # Share embedding\n",
    "    self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "    \n",
    "    # Optionally tie weights as in:\n",
    "    # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "    if tie_weights_bolean:\n",
    "      # Depend of the configuration you choose, the dimmensions must be : E= H or 2*H ou 3*H\n",
    "      self.decoder.out.weight = self.encoder.embedding.weight\n",
    "    \n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:self.decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    E_hist = None\n",
    "    decoded_words = [[] for y in range(batch_size)]\n",
    "    max_target_length = max(target_lengths)\n",
    "    hidden_history_decoder = decoder_hidden[-1].unsqueeze(0)\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    # Run through decoder one time step at a time\n",
    "    \n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn, E_hist,hidden_history_decoder = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs,E_hist,t, hidden_history_decoder, input_batches\n",
    "        )\n",
    "        \n",
    "\n",
    "          \n",
    "        #REMPLACER ni par [ni]*Batch (pour chaque batch déterminer ni)\n",
    "        \n",
    "        if pointeur_bolean:\n",
    "          [decoder_output_voc, decoder_output_context, all_decoder_outputs_pointer ]=decoder_output\n",
    "          \n",
    "          if all_decoder_outputs_pointer.cpu().data.numpy()[0][0]>0.5:\n",
    "            topv, topi = decoder_output_context.data.topk(1)\n",
    "            decoder_input = Variable(topi.squeeze(1))\n",
    "            for batch, val  in enumerate(decoder_input):\n",
    "              try:\n",
    "                decoded_words[batch].append(input_letters[batch][val.cpu().data[0]])\n",
    "              except Exception:\n",
    "                 decoded_words[batch].append('UNK')\n",
    "              \n",
    "\n",
    "         #cest chaud\n",
    "          else:\n",
    "            topv, topi = decoder_output_voc.data.topk(1)\n",
    "            decoder_input = Variable(topi.squeeze(1))\n",
    "            for batch, val  in enumerate(decoder_input):\n",
    "              decoded_words[batch].append(indextoword[val.cpu().data[0]])\n",
    "         \n",
    "        else:        \n",
    "          # Choose top word from output\n",
    "          topv, topi = decoder_output.data.topk(1)\n",
    "          #decoded_words.append(indextoword[ni])\n",
    "          decoder_input = Variable(topi.squeeze(1))\n",
    "          for batch, val  in enumerate(decoder_input):\n",
    "            decoded_words[batch].append(indextoword[val.cpu().data[0]])\n",
    "        if USE_CUDA:\n",
    "          decoder_input = decoder_input.cuda()\n",
    "    final_in=[]\n",
    "    final_out=[]\n",
    "    for batch, val  in enumerate(decoder_input):\n",
    "      final_in.append(' '.join(input_letters[batch]))\n",
    "      final_out.append(' '.join(decoded_words[batch]))  \n",
    "    total_loss=reward_predictor.eval(final_in, final_out)\n",
    "    total_loss=total_loss.sum()\n",
    "    total_loss.backward()\n",
    "      \n",
    "    # Clip gradient norms\n",
    "    ec = torch.nn.utils.clip_grad_norm(self.encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm(self.decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return total_loss.data[0], ec, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZNUx_jKbFDyG"
   },
   "outputs": [],
   "source": [
    "class RewardPredictorTrainer:\n",
    "    \"\"\"\n",
    "    Model used for the human preferences metric\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, embed_size, hidden_size, output_size, indextoword,\n",
    "                 wordtoindex, num_layer_encoder=1, num_layer_decoder=1, \n",
    "                 dropout=0.5, SOS_token=1, EOS_token=2, PAD_token=0, batch_size=5, article_max_size=500):\n",
    "        \"\"\"\n",
    "        :param input_size:\n",
    "        :param embed_size:\n",
    "        :param hidden_size:\n",
    "        :param output_size:\n",
    "        :param indextoword:\n",
    "        :param wordtoindex\n",
    "        :param num_layer_encoder:\n",
    "        :param num_layer_decoder:\n",
    "        :param dropout:\n",
    "        :param SOS_token:\n",
    "        :param EOS_token:\n",
    "        :param PAD_token:\n",
    "        :param batch_size:\n",
    "        :param article_max_size:\n",
    "        \"\"\"\n",
    "        self.encoder = EncoderRNN(input_size, embed_size, hidden_size, pretrained_weight, num_layer_encoder, dropout)\n",
    "        self.decoder = DecoderStep(hidden_size, embed_size, output_size, num_layer_decoder, temporal=False,\n",
    "                                   de_att_bol=False, point_bol=False, dropout_p=dropout, attention_bol=False)\n",
    "        self.predictor = RewardPredictor(input_size, embed_size, hidden_size, \n",
    "                                         indextoword,wordtoindex, num_layer_encoder, \n",
    "                                         dropout, SOS_token, EOS_token, PAD_token, \n",
    "                                         batch_size, article_max_size)\n",
    "        self.indextoword = indextoword\n",
    "        self.wordtoindex = wordtoindex\n",
    "        self.SOS_token = SOS_token\n",
    "        self.EOS_token = EOS_token\n",
    "        self.PAD_token = PAD_token\n",
    "        self.batch_size = batch_size\n",
    "        self.article_max_size = article_max_size\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()\n",
    "            self.predictor.cuda()\n",
    "\n",
    "    def train_step_autoenc(self, input_batches, input_lengths,\n",
    "              encoder_optimizer, decoder_optimizer, criterion, \n",
    "              max_length=1000, E_hist=None):\n",
    "        \"\"\"\n",
    "        Train step for the autoencoder\n",
    "        \"\"\"\n",
    "        # The targets are the inputs\n",
    "        target_batches = input_batches\n",
    "        target_lengths = input_lengths\n",
    "        # Zero gradients of both optimizers\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0 # Added onto for each word\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_batches, input_lengths, None)\n",
    "\n",
    "        # Share embedding\n",
    "        self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "\n",
    "        # Prepare input and output variables\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "        max_target_length = max(target_lengths)\n",
    "        all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, self.decoder.output_size))\n",
    "        hidden_history_decoder = decoder_hidden[-1].unsqueeze(0)\n",
    "\n",
    "        # Move new Variables to CUDA\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "        # Run through decoder one time step at a time\n",
    "        for t in range(max_target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn, E_hist,hidden_history_decoder = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs,E_hist,t, hidden_history_decoder, input_batches\n",
    "            )\n",
    "\n",
    "            all_decoder_outputs[t] = decoder_output\n",
    "            decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "        # Loss calculation and backpropagation\n",
    "        loss = masked_cross_entropy(\n",
    "            all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "            target_lengths\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradient norms\n",
    "        ec = torch.nn.utils.clip_grad_norm(self.encoder.parameters(), clip)\n",
    "        dc = torch.nn.utils.clip_grad_norm(self.decoder.parameters(), clip)\n",
    "\n",
    "        # Update parameters with optimizers\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        return loss.data[0], ec, dc\n",
    "      \n",
    "    def train_autoenc(self, learning_rate, decoder_learning_ratio, n_epochs, batch_size, \n",
    "              pairs_train, wordtoindex):\n",
    "      \"\"\"\n",
    "      Training of the autoencoder\n",
    "      \"\"\"\n",
    "      encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "      decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      # Keep track of time elapsed and running averages\n",
    "      start = time.time()\n",
    "      plot_losses = []\n",
    "      print_loss_total = 0 # Reset every print_every\n",
    "      plot_loss_total = 0 # Reset every plot_every\n",
    "      ecs = []\n",
    "      dcs = []\n",
    "      eca = 0\n",
    "      dca = 0\n",
    "      epoch = 0\n",
    "      \n",
    "      while epoch < n_epochs:\n",
    "          epoch += 1\n",
    "\n",
    "          print(epoch)\n",
    "\n",
    "          #print (epoch)\n",
    "          # Get training data for this cycle\n",
    "          input_batches, input_lengths, target_batches, target_lengths,target_context, target_pointer,_ = random_batch(batch_size,pairs_train, wordtoindex)\n",
    "          # Run the train function\n",
    "          \"\"\"\n",
    "          self, input_batches, input_lengths, target_lengths,\n",
    "              encoder_optimizer, decoder_optimizer, criterion, \n",
    "              max_length=1000, E_hist=None):\n",
    "              \"\"\"\n",
    "          loss, ec, dc = self.train_step_autoenc(\n",
    "              input_batches, input_lengths,\n",
    "              encoder_optimizer, decoder_optimizer, criterion\n",
    "          )\n",
    "\n",
    "          # Keep track of loss\n",
    "          print_loss_total += loss\n",
    "          plot_loss_total += loss\n",
    "          eca += ec\n",
    "          dca += dc\n",
    "\n",
    "          if epoch == 5:\n",
    "              self.evaluate_randomly()\n",
    "\n",
    "          if epoch % print_every == 0:\n",
    "              print_loss_avg = print_loss_total / print_every\n",
    "              print_loss_total = 0\n",
    "              print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "              print(print_summary)\n",
    "              show_losses(ecs,dcs,plot_losses)\n",
    "\n",
    "          if epoch % evaluate_every == 0:\n",
    "              self.evaluate_randomly()\n",
    "\n",
    "          if epoch % plot_every == 0:\n",
    "              plot_loss_avg = plot_loss_total / plot_every\n",
    "              plot_losses.append(plot_loss_avg)\n",
    "              plot_loss_total = 0\n",
    "\n",
    "              # TODO: Running average helper\n",
    "              ecs.append(eca / plot_every)\n",
    "              dcs.append(dca / plot_every)\n",
    "\n",
    "              eca = 0\n",
    "              dca = 0\n",
    "              plot_loss_total = 0\n",
    "              \n",
    "    def evaluate_and_show_attention(self, input_sentence, target_sentence=None):\n",
    "      output_words, attentions = self.eval_autoenc(input_sentence)\n",
    "      output_sentence = ' '.join(output_words)\n",
    "      print('>', input_sentence)\n",
    "      if target_sentence is not None:\n",
    "          print('=', target_sentence)\n",
    "      print('<', output_sentence)\n",
    "\n",
    "      #show_attention(input_sentence, output_words, attentions)\n",
    "  \n",
    "  \n",
    "    def evaluate_randomly(self):\n",
    "      [input_sentence, target_sentence] = random.choice(pairs_train)\n",
    "      self.evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "\n",
    "\n",
    "    def eval_autoenc(self, input_seq, training_mode=False, max_length=50):\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        input_lengths = [len(input_seq.split())]\n",
    "        \n",
    "        input_seq = indexes_from_sentence(self.wordtoindex, input_seq)\n",
    "        input_batches = Variable(torch.LongTensor([input_seq]), volatile=True).transpose(0, 1)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            input_batches = input_batches.cuda()\n",
    "\n",
    "        # Set to not-training mode to disable dropout\n",
    "        if not training_mode:\n",
    "            self.encoder.train(False)\n",
    "            self.decoder.train(False)\n",
    "\n",
    "        # Run through encoder\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_batches, input_lengths, None)\n",
    "\n",
    "        # Create starting vectors for self.decoder\n",
    "        decoder_input = Variable(torch.LongTensor([self.SOS_token]), volatile=True)  # SOS\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]  # Use last (forward) hidden state from self.encoder\n",
    "        hidden_history_decoder = decoder_hidden[-1].unsqueeze(0)\n",
    "        E_hist = None\n",
    "        if USE_CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "\n",
    "        # Store output words and attention states\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "\n",
    "        # Run through decoder\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention, E_hist, hidden_history_decoder = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, E_hist, di, hidden_history_decoder, input_batches\n",
    "            )\n",
    "            #decoder_attentions[di, :decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "            # Choose top word from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni == self.EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(self.indextoword[ni])\n",
    "\n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([ni]))\n",
    "            if USE_CUDA:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "        if not training_mode:\n",
    "            # Set back to training mode\n",
    "            self.encoder.train(True)\n",
    "            self.decoder.train(True)\n",
    "        return decoded_words, encoder_outputs\n",
    "      \n",
    "    def train_step_predictor(self, input_batches, input_length, input_ref_batches, input_ref_length):\n",
    "        \"\"\"\n",
    "        Step to train the reward predictor\n",
    "        \"\"\"\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        \n",
    "        input_batches = Variable(torch.LongTensor([input_batches])).transpose(0, 1)\n",
    "        input_batches_ref = Variable(torch.LongTensor([input_ref_batches])).transpose(0, 1)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_batches = input_batches.cuda()\n",
    "            input_batches_ref = input_batches_ref.cuda()\n",
    "            \n",
    "        \n",
    "        return self.predictor(input_batches_ref, input_ref_length, input_batches, input_length)   \n",
    "      \n",
    "\n",
    "    def eval(self, input_seq_ref, input_seq, is_training=False):\n",
    "        \"\"\"\n",
    "        Evaluate the reward function\n",
    "        :param input_seq_ref: ref sequence\n",
    "        :param input_seq: summary sequence\n",
    "        \"\"\"\n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        to_indexes_fn = lambda x: indexes_from_sentence(self.wordtoindex, x)\n",
    "        input_seq = list(map(to_indexes_fn, input_seq))\n",
    "        \n",
    "        input_batches_ref = list(map(to_indexes_fn, input_seq_ref))\n",
    "        input_seq = sorted(input_seq, key=lambda p: len(p), reverse=True)\n",
    "        input_batches_ref = sorted(input_batches_ref, key=lambda p: len(p), reverse=True)\n",
    "        \n",
    "        input_lengths = list(map(len, input_seq))\n",
    "        input_lengths_ref = list(map(len, input_batches_ref))\n",
    "        \n",
    "        input_seq = [pad_seq(s, max(input_lengths)) for s in input_seq]\n",
    "        input_batches_ref = [pad_seq(s, max(input_lengths_ref)) for s in input_batches_ref]\n",
    "        \n",
    "        input_batches = Variable(torch.LongTensor(input_seq)).transpose(0, 1)\n",
    "        input_batches_ref = Variable(torch.LongTensor(input_batches_ref)).transpose(0, 1)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            input_batches = input_batches.cuda()\n",
    "            input_batches_ref = input_batches_ref.cuda()\n",
    "\n",
    "        # Set to not-training mode to disable dropout\n",
    "        if not is_training:\n",
    "            self.predictor.train(False)\n",
    "        \n",
    "        print('input_batches_ref.size()')\n",
    "        print(input_batches_ref.size())\n",
    "        print('input_lengths_ref.size()')\n",
    "        print(input_lengths_ref)\n",
    "        print('input_batches.size()')\n",
    "        print(input_batches.size())\n",
    "        print('input_lengths.size()')\n",
    "        print(input_lengths)\n",
    "        loss = self.predictor(input_batches_ref, input_lengths_ref, input_batches, input_lengths)\n",
    "        \n",
    "        if not is_training:\n",
    "            self.predictor.train(True)\n",
    "          \n",
    "        return loss\n",
    "        \n",
    "    def train(self, input_seq_ref, input_seq, input_seq_ref_2, input_seq_2, proba_density, n_epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Train the reward predictor\n",
    "        :param input_seq_ref: article 1\n",
    "        :param input_seq: summary 1\n",
    "        :param input_seq_ref_2: article 2\n",
    "        :param input_seq_2: summary 2\n",
    "        :param proba_density: list of list [1, 0] if first is better, [0, 1] if second is better, [0.5, 0.5] is equally good\n",
    "        :param n_epochs:\n",
    "        :param learning_rate:\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.predictor.parameters(), lr=learning_rate)\n",
    "        \n",
    "        USE_CUDA = torch.cuda.is_available()\n",
    "        \n",
    "        all_input_sequences = input_seq\n",
    "        all_ref_sequences = input_seq_ref\n",
    "        all_input_sequences_2 = input_seq_2\n",
    "        all_ref_sequences_2 = input_seq_ref_2\n",
    "        all_target_scores = proba_density\n",
    "        \n",
    "        epoch = 0\n",
    "        print_loss_total = 0\n",
    "        plot_loss_total = 0\n",
    "        eca = 0\n",
    "        \n",
    "        \n",
    "        while epoch < n_epochs:\n",
    "            epoch += 1\n",
    "\n",
    "            print(\"Epoch:\", epoch)\n",
    "\n",
    "            #print (epoch)\n",
    "            # Get training data for this cycle\n",
    "            input_seq, input_length, input_seq_ref, input_ref_length, order_batch1, \\\n",
    "                input_seq_2, input_length_2, input_seq_ref_2, \\\n",
    "                input_ref_length_2, order_batch2, target_scores = random_batch_three(self.batch_size, all_input_sequences, all_ref_sequences, all_input_sequences_2, all_ref_sequences_2,  all_target_scores, self.wordtoindex)\n",
    "            # Run the train function\n",
    "            \"\"\"\n",
    "            self, input_batches, input_lengths, target_lengths,\n",
    "              encoder_optimizer, decoder_optimizer, criterion, \n",
    "              max_length=1000, E_hist=None):\n",
    "              \"\"\"\n",
    "            \n",
    "            target_scores = Variable(torch.Tensor(target_scores)).transpose(0, 1)\n",
    "            P1_2 = Variable(torch.Tensor(self.batch_size))\n",
    "            P2_1 = Variable(torch.Tensor(self.batch_size))\n",
    "        \n",
    "            if USE_CUDA:\n",
    "                target_scores = target_scores.cuda()\n",
    "                P1_2 = P1_2.cuda()\n",
    "                P2_1 = P2_1.cuda()\n",
    "            \n",
    "            # Zero gradients of both optimizers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output1 = self.train_step_predictor(\n",
    "              input_seq, input_length, input_seq_ref, input_ref_length\n",
    "            )\n",
    "            \n",
    "            output2 = self.train_step_predictor(\n",
    "              input_seq_2, input_length_2, input_seq_ref_2, input_ref_length_2\n",
    "            )\n",
    "            \n",
    "            # Calculate probas\n",
    "            for k in range(self.batch_size):\n",
    "                index_1 = order_batch1.index(k)\n",
    "                index_2 = order_batch2.index(k)\n",
    "                P1_2[k] = output1[index_1] / (output1[index_1] + output2[index_2])\n",
    "                P2_1[k] = output2[index_2] / (output1[index_1] + output2[index_2])\n",
    "            # Calculate loss\n",
    "            loss = torch.sum(- target_scores[0] * torch.log(P1_2) - target_scores[1] * torch.log(P2_1))            \n",
    "\n",
    "            # Loss calculation and backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient norms\n",
    "            c = torch.nn.utils.clip_grad_norm(self.predictor.parameters(), clip)\n",
    "\n",
    "            # Update parameters with optimizers\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if epoch % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "                print(print_summary)\n",
    "                show_losses(None,None,plot_losses)\n",
    "\n",
    "            if epoch % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                plot_loss_total = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFoDIT11ECrI"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "agwtzyGRECHy"
   },
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if USE_CUDA:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    if USE_CUDA:\n",
    "      seq_length_expand=seq_length_expand.cuda()\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(torch.LongTensor(length))\n",
    "    if USE_CUDA:\n",
    "      length=length.cuda()\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YDWirPFsESG-"
   },
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs_train)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)\n",
    "    \n",
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = Seq2SEq_main_model.evaluate(input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    \n",
    "    show_attention(input_sentence, output_words, attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WVMEmAlnEnTe"
   },
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def show_losses(y1,y2,y3):\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    if y1 is not None:\n",
    "        ax1 = fig.add_subplot(1, 3, 1)\n",
    "        ax1.plot(y1, label='data 1')\n",
    "        ax1.set_xlabel('epochs')\n",
    "        ax1.set_ylabel('encoder grad')\n",
    "        ax1.set_title('GRAD ENCODER WITH EPOCH')\n",
    "        ax1.legend()\n",
    "    if y2 is not None:\n",
    "        ax2 = fig.add_subplot(1, 3, 2)\n",
    "        ax2.plot(y2, label='data 2')\n",
    "        ax2.set_xlabel('epochs')\n",
    "        ax2.set_ylabel('epochs')\n",
    "        ax2.set_title('GRAD DECODER WITH EPOCH')\n",
    "        ax2.legend()\n",
    "    if y3 is not None:\n",
    "        ax3 = fig.add_subplot(1, 3, 3)\n",
    "        ax3.plot(y3, label='data 3')\n",
    "        ax3.set_xlabel('epochs')\n",
    "        ax3.set_title('LOSS WITH EPOCH')\n",
    "        ax3.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejK2E5rOFTPw"
   },
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1304,
     "status": "ok",
     "timestamp": 1522492875270,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "BcaenW4fP1lM",
    "outputId": "c64ef05e-6ab3-4f52-de76-676ab8f506cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 500\n",
      "size reward 144 50\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "attn_model = 'concat'\n",
    "hidden_size = 300\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 10\n",
    "embed_size=100\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 50000\n",
    "plot_every = 20\n",
    "print_every = 100\n",
    "encoder_temporal = True\n",
    "pointeur_bolean=True\n",
    "attention_bolean=True\n",
    "decoder_attention_bol = True\n",
    "tie_weights_bolean=False\n",
    "MAX_LENGTH_OUTPUT_GENERATE=100\n",
    "\n",
    "Seq2SEq_main_model= Seq2SeqTrainer(attn_model, hidden_size, embed_size, n_layers, wordtoindex, indextoword,\n",
    "                     n_epochs, encoder_temporal,decoder_attention_bol, pairs_train,\n",
    "                      pointeur_bolean, attention_bolean, tie_weights_bolean, num_layer_encoder=1,\n",
    "                      num_layer_decoder=1,dropout=0.5, SOS_token=1, EOS_token=2, PAD_token=0, batch_size=5,\n",
    "                      clip=50,learning_rate = 0.0001, article_max_size=500, decoder_learning_ratio=5,\n",
    "                      plot_every=20, print_every = 100, evaluate_every = 1000, max_length=MAX_LENGTH_OUTPUT_GENERATE)\n",
    "\n",
    "reward_predictor= RewardPredictorTrainer(len(indextoword), embed_size, hidden_size, len(indextoword), indextoword,\n",
    "                 wordtoindex, num_layer_encoder=1, num_layer_decoder=1, \n",
    "                 dropout=0.5, SOS_token=1, EOS_token=2, PAD_token=0, batch_size=5, article_max_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DGPEm-7kkYtA"
   },
   "outputs": [],
   "source": [
    "#Seq2SEq_main_model.train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x20UerwOFf1I"
   },
   "source": [
    "###3/Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HW7R_rlVFSnu"
   },
   "outputs": [],
   "source": [
    "# reward_predictor.train_autoenc(learning_rate, decoder_learning_ratio, n_epochs, batch_size, pairs_train, wordtoindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOuCZX4E9Dko"
   },
   "source": [
    "# Our model Scrib-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5401,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      },
      {
       "item_id": 7
      },
      {
       "item_id": 8
      },
      {
       "item_id": 12
      },
      {
       "item_id": 13
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60078,
     "status": "error",
     "timestamp": 1522492939648,
     "user": {
      "displayName": "Benamira Adrien",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104422021836962067229"
     },
     "user_tz": -120
    },
    "id": "4d8OQtGM5EJ2",
    "outputId": "b9699a95-629c-4a3f-d6d9-72c261d455e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batches_ref.size()\n",
      "torch.Size([1248, 5])\n",
      "input_lengths_ref.size()\n",
      "[1248, 1068, 1016, 686, 592]\n",
      "input_batches.size()\n",
      "torch.Size([66, 5])\n",
      "input_lengths.size()\n",
      "[66, 66, 66, 66, 66]\n",
      "sortie Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.1014  0.3168  0.0562  ...   0.5845 -0.1397 -0.0138\n",
      "  0.0614 -0.1577  0.3241  ...   0.6806 -0.0143 -0.2096\n",
      "  0.0975 -0.3749  0.2823  ...   0.2251 -0.2385 -0.1637\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.3025 -0.0838  0.0266  ...   0.1071 -0.5655  0.2011\n",
      "  0.3436  0.1294 -0.0351  ...   0.0787  0.1367  0.0759\n",
      " -0.0655  0.1260 -0.0605  ...  -0.0264  0.3545  0.1125\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 2 ,.,.) = \n",
      " -0.4098 -0.1046  0.1253  ...   0.1624 -0.4032  0.2024\n",
      " -0.4146 -0.0419 -0.3684  ...   0.6923 -0.0621 -0.1958\n",
      " -0.2936 -0.1612 -0.1588  ...   0.6008 -0.6838  0.1439\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 3 ,.,.) = \n",
      " -0.0495  0.2945  0.2642  ...   0.0817  0.0101  0.2412\n",
      "  0.1740  0.4708  0.2830  ...  -0.0285 -0.1717 -0.2572\n",
      "  0.4277  0.1993  0.3141  ...   0.0642 -0.3200 -0.3408\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 4 ,.,.) = \n",
      "  0.1025 -0.1244 -0.1010  ...  -0.1470 -0.1167 -0.4064\n",
      "  0.4436  0.0531  0.2304  ...   0.0184 -0.2854  0.0197\n",
      "  0.5219  0.0528  0.0056  ...   0.2459 -0.1604  0.1041\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x500x300]\n",
      "\n",
      "avant\n",
      "torch.Size([5, 1, 1000, 300])\n",
      "après C1\n",
      "torch.Size([5, 8, 124, 36])\n",
      "après C2\n",
      "torch.Size([5, 8, 20, 5])\n",
      "après C3\n",
      "torch.Size([5, 8, 8, 1])\n",
      "maitenant\n",
      "torch.Size([5, 1, 64])\n",
      "Epoch: 2\n",
      "input_batches_ref.size()\n",
      "torch.Size([1594, 5])\n",
      "input_lengths_ref.size()\n",
      "[1594, 1324, 1068, 1068, 408]\n",
      "input_batches.size()\n",
      "torch.Size([66, 5])\n",
      "input_lengths.size()\n",
      "[66, 66, 66, 66, 66]\n",
      "sortie Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.1007  0.1164  0.0052  ...   0.2372  0.2884  0.2648\n",
      " -0.0700 -0.2346 -0.0446  ...   0.5648  0.0380  0.1671\n",
      "  0.2790  0.1364 -0.1905  ...   0.2472  0.2493 -0.1626\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 1 ,.,.) = \n",
      " -0.3137  0.1614 -0.1982  ...  -0.2255  0.2810 -0.1092\n",
      " -0.0067  0.1789 -0.2255  ...  -0.1404  0.2336 -0.2858\n",
      "  0.2345 -0.1332 -0.0729  ...   0.2267  0.2901 -0.4463\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 2 ,.,.) = \n",
      " -0.3060  0.3041 -0.2733  ...   0.0521  0.2255  0.2660\n",
      "  0.1438 -0.0707 -0.2899  ...  -0.2668 -0.3882 -0.2731\n",
      " -0.0969  0.1245 -0.3167  ...  -0.2787 -0.1999  0.1669\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 3 ,.,.) = \n",
      " -0.0645  0.0303  0.1790  ...  -0.3668  0.2440  0.0181\n",
      "  0.1490 -0.1453  0.2091  ...  -0.1609  0.4348 -0.0914\n",
      " -0.1695 -0.0658 -0.1520  ...  -0.0149  0.1829  0.3761\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 4 ,.,.) = \n",
      "  0.0713 -0.2687  0.0548  ...   0.2379  0.1823  0.0310\n",
      "  0.2009  0.1759  0.1173  ...   0.0257  0.0077 -0.0572\n",
      "  0.2641  0.1928 -0.0752  ...  -0.0951 -0.4448 -0.1006\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x500x300]\n",
      "\n",
      "avant\n",
      "torch.Size([5, 1, 1000, 300])\n",
      "après C1\n",
      "torch.Size([5, 8, 124, 36])\n",
      "après C2\n",
      "torch.Size([5, 8, 20, 5])\n",
      "après C3\n",
      "torch.Size([5, 8, 8, 1])\n",
      "maitenant\n",
      "torch.Size([5, 1, 64])\n",
      "> ( cnn ) -- if golf has a reputation for being a bit stuffy , then the bryan brothers and their trick shots are a much-needed blast of fresh air . hailing from south carolina , george and younger brother wesley have been attracting a growing band of followers with a colorful array of videos . from casually shooting hoops with a wedge to smashing shots from mid-air in the dark , the siblings are hoping to show off golf in a new light and attract a younger generation of players back to the sport . `` we love golf , but most people think it 's kind of boring , '' george bryan told cnn . `` we 're providing something that 's a little different . we 're bringing a kind of hip , fun way to view golf and hope that it brings in a younger demographic . we want them to say : 'that 's fun , i wan na go try the game ' , '' the 26-year-old added . both brothers are qualified professionals and started filming trick shots as a hobby earlier this year , but it has quickly grown into a mini-industry after racking up tens of thousands of clicks on their website and youtube . `` most of the tricks are done on the fly , '' explains 24-year-old wesley , who smashes the balls set up by george . `` a lot of them work on the first or second try . if the ball comes into my hit zone then it normally goes pretty quick -- a lot quicker than what people say on the comments board , '' he added . the pair started out experimenting at woodcreek & wildewood , their local club in state capital columbia , but have since diversified their content . in the summer , a football-themed `` world cup edition '' celebrated this year 's tournament in brazil and collaborations with basketball trick shot specialists `` the legendary shots '' and pro golfer and model blair o'neal have widened their appeal . the brothers are not the only trick shot artists to showcase their skills online . french golf professional , romain bechu 's incredible ball-juggling skills has received over half-a-million views on youtube and british trick shot specialist david edwards has performed his routine at dozens of pro events including the open championship and the ryder cup . when the bryan brothers are not dreaming up new routines , they compete on the national golf association ( nga ) pro golf tour and the web.com tour , a developmental tour for the main pga tour -- which is where the pair hope to end up playing one day . `` the goal is really to use this to help our pro golf careers out , but at the same time we view this as a hobby , we both enjoy it , '' george says . `` and when we do have an off week , it 's good to get out minds off tournament golf and there 's no better outlet to be creative than trying some trick shots out and seeing how far we can take it . '' watch : spieth recreates amazing shot\n",
      "= golfing siblings attracting a growing following online with novel trick shots . pair from south carolina aiming to make golf appeal to younger audience . collaborations and themed clips have expanded brothers ' range of tricks . both are qualified professionals and hope to play on pga tour one da .\n",
      "< wireless naturally lauren lauren 1 virgin accuse the goal accuse accuse goal accuse accuse goal approval accuse accuse but videos videos hunt players the artists literacy rice had had accuse cotton accuse accuse first first new it accuse accuse new accuse new all-time . this shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots shots '' '' '' cantor cantor ron up up and and and and and and and and and hope hope and we and year year and this and fly and year and and and and and\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAADnCAYAAAAElsEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQEiSmgwISVLT/wBfuTdJrSSw2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe501d5d2b0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "input_batches_ref.size()\n",
      "torch.Size([1324, 5])\n",
      "input_lengths_ref.size()\n",
      "[1324, 1016, 717, 517, 215]\n",
      "input_batches.size()\n",
      "torch.Size([65, 5])\n",
      "input_lengths.size()\n",
      "[65, 65, 65, 65, 65]\n",
      "sortie Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.1835 -0.2821 -0.0261  ...   0.1021 -0.0962 -0.0724\n",
      "  0.2447  0.0997 -0.2825  ...  -0.2876  0.1549 -0.3266\n",
      "  0.5145 -0.0128 -0.1849  ...  -0.1512  0.2643 -0.2028\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.0888 -0.4698 -0.4927  ...   0.0114  0.2210  0.3433\n",
      " -0.0397 -0.6504 -0.3228  ...   0.1614 -0.0925  0.0395\n",
      "  0.0146 -0.0569 -0.4609  ...  -0.0954  0.0547  0.3048\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 2 ,.,.) = \n",
      " -0.0195 -0.3299 -0.5048  ...  -0.1479  0.2008  0.2466\n",
      " -0.3335 -0.1808 -0.5304  ...   0.0349  0.0998 -0.2335\n",
      " -0.4251 -0.1888 -0.5829  ...   0.1321 -0.0174 -0.2073\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 3 ,.,.) = \n",
      "  0.1850 -0.1315  0.1454  ...   0.0263  0.3856  0.0658\n",
      " -0.2550  0.0865 -0.1805  ...  -0.0751 -0.0167  0.6105\n",
      "  0.0386 -0.3755 -0.3049  ...  -0.1432 -0.0438  0.5569\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 4 ,.,.) = \n",
      "  0.0150  0.0900 -0.2328  ...  -0.2086 -0.2631  0.0305\n",
      " -0.0336 -0.0107  0.2731  ...  -0.0326 -0.1953  0.0905\n",
      "  0.1282 -0.1135  0.4470  ...   0.0906 -0.0404 -0.0403\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x500x300]\n",
      "\n",
      "avant\n",
      "torch.Size([5, 1, 1000, 300])\n",
      "après C1\n",
      "torch.Size([5, 8, 124, 36])\n",
      "après C2\n",
      "torch.Size([5, 8, 20, 5])\n",
      "après C3\n",
      "torch.Size([5, 8, 8, 1])\n",
      "maitenant\n",
      "torch.Size([5, 1, 64])\n",
      "Epoch: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1e30ac790914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     loss, ec, dc = Seq2SEq_main_model.train_master_piece(\n\u001b[1;32m     23\u001b[0m         \u001b[0minput_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mencoder_total_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_total_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_letters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-80ad0efa6263>\u001b[0m in \u001b[0;36mtrain_master_piece\u001b[0;34m(self, input_batches, input_lengths, encoder_optimizer, decoder_optimizer, input_letters, reward_predictor)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         decoder_output, decoder_hidden, decoder_attn, E_hist,hidden_history_decoder = self.decoder(\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mE_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_history_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5117a3ec8d11>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, last_hidden, encoder_outputs, E_hist, t, hd_history, input_batches)\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0;31m# Calculate attention weights -temporal or not- of encoder (alpha) and apply to encoder outputs (context_encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m               \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_hist\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,1,T) (1,B,T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m               \u001b[0mE_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d2725468886f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs, E_history)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B*T*H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mattn_energies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mE_history\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d2725468886f>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'concat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B*T*2H]->[B*T*H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B*H*T]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B*1*H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(tensor1, tensor2, out)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "epoch=0\n",
    "print_loss_total= 0\n",
    "plot_loss_total= 0\n",
    "evaluate_every = 1000\n",
    "plot_every = 20\n",
    "batch_size=5\n",
    "encoder_total_optimizer = optim.Adam(Seq2SEq_main_model.encoder.parameters(), lr=learning_rate)\n",
    "decoder_total_optimizer = optim.Adam(Seq2SEq_main_model.decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    print(\"Epoch: \" + str(epoch))\n",
    "\n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths,target_context, target_pointer, input_letters = random_batch(batch_size,pairs_train, wordtoindex)\n",
    "    \n",
    "    # Run the train function\n",
    "    loss, ec, dc = Seq2SEq_main_model.train_master_piece(\n",
    "        input_batches, input_lengths,\n",
    "        encoder_total_optimizer, decoder_total_optimizer,input_letters, reward_predictor\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "\n",
    "    if epoch == 2:\n",
    "        evaluate_randomly()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        show_losses(ecs,dcs,plot_losses)\n",
    "        \n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)        \n",
    "        eca = 0\n",
    "        dca = 0\n",
    "        plot_loss_total = 0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Seq2Seq_pointeur.ipynb",
   "provenance": [
    {
     "file_id": "1cxAKfKG_ezHuvC4g3f-Gie0sy7oQuniH",
     "timestamp": 1520720633992
    },
    {
     "file_id": "1kUCJWtDOIPgfU2afJHmihCrLbFCES40J",
     "timestamp": 1520694626938
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
